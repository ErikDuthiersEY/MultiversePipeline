{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020c99c4",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc52a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4f0ff",
   "metadata": {},
   "source": [
    "Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e2fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_DIR: c:/Users/WJ724NE/OneDrive - EY/Documents/MultiversePipeline/datasets/raw/\n",
      "OUT_DIR: c:/Users/WJ724NE/OneDrive - EY/Documents/MultiversePipeline/datasets/processed/\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = Path.cwd().parent\n",
    "RAW_DIR = ROOT_DIR / \"datasets\" / \"raw\"\n",
    "OUT_DIR = ROOT_DIR / \"datasets\" / \"processed\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"RAW_DIR: {RAW_DIR.as_posix()}/\")\n",
    "print(f\"OUT_DIR: {OUT_DIR.as_posix()}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291cb5d",
   "metadata": {},
   "source": [
    "Function to convert .csv to .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75ace329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVs encontrados: 7\n",
      " - bias_dataset.csv\n",
      " - hallucination.csv\n",
      " - reasoning_close.csv\n",
      " - reasoning_open.csv\n",
      " - refusal_correctness.csv\n",
      " - summarization.csv\n",
      " - variation_sensitivity_with_typos.csv\n",
      "Éxito: bias_dataset.csv (enc=utf-8, sep=',')\n",
      "OK: bias_dataset.csv → bias_dataset.parquet\n",
      "Éxito: hallucination.csv (enc=utf-8, sep=',')\n",
      "OK: hallucination.csv → hallucination.parquet\n",
      "Éxito: reasoning_close.csv (enc=utf-8, sep=',')\n",
      "OK: reasoning_close.csv → reasoning_close.parquet\n",
      "Éxito: reasoning_open.csv (enc=utf-8, sep=',')\n",
      "OK: reasoning_open.csv → reasoning_open.parquet\n",
      "Éxito: refusal_correctness.csv (enc=utf-8, sep=',')\n",
      "OK: refusal_correctness.csv → refusal_correctness.parquet\n",
      "Éxito: summarization.csv (enc=utf-8, sep=',')\n",
      "OK: summarization.csv → summarization.parquet\n",
      "Éxito: variation_sensitivity_with_typos.csv (enc=utf-8, sep=',')\n",
      "Transformando variation_sensitivity_with_typos.csv → formato largo (3 filas por ejemplo)\n",
      "→ 100 ejemplos originales → 300 filas (x3)\n",
      "OK: variation_sensitivity_with_typos.csv → variation_sensitivity.parquet\n",
      "\n",
      "Total Parquets generados: 7\n",
      "   • bias_dataset.parquet\n",
      "   • hallucination.parquet\n",
      "   • reasoning_close.parquet\n",
      "   • reasoning_open.parquet\n",
      "   • refusal_correctness.parquet\n",
      "   • summarization.parquet\n",
      "   • variation_sensitivity.parquet\n"
     ]
    }
   ],
   "source": [
    "def read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"latin-1\", \"cp1252\"]\n",
    "    delimiters = [',', ';', '\\t']\n",
    "    last_err = None\n",
    "    \n",
    "    for enc in encodings:\n",
    "        for delim in delimiters:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=delim, engine=\"python\", encoding=enc, quoting=0, on_bad_lines='warn')\n",
    "                print(f\"Éxito: {path.name} (enc={enc}, sep='{delim}')\")\n",
    "                if len(df.columns) < 3:\n",
    "                    raise ValueError(\"Demasiado pocas columnas detectadas\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                print(f\"Intento fallido: {path.name} (enc={enc}, sep='{delim}'): {e}\")\n",
    "    \n",
    "    # Fallback\n",
    "    print(f\"Fallback para {path.name}\")\n",
    "    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        reader = csv.reader(f, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        rows = list(reader)\n",
    "    if not rows:\n",
    "        raise ValueError(\"Archivo vacío\")\n",
    "    columns = [str(c).strip() for c in rows[0]]\n",
    "    data = [row for row in rows[1:] if len(row) == len(columns)]\n",
    "    if not data:\n",
    "        raise ValueError(\"Sin datos válidos\")\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['id', 'index']:  \n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "# transformar variation_sensitivity_with_typos\n",
    "\n",
    "def transform_variation_sensitivity(df: pd.DataFrame, csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte el CSV con columnas:\n",
    "    id, category, difficulty, prompt_original, prompt_paraphrase, prompt_typo_variant\n",
    "    → 3 filas por ejemplo con columnas: id, prompt, group_id, variation_type, category, difficulty\n",
    "    \"\"\"\n",
    "    print(f\"Transformando {csv_path.name} → formato largo (3 filas por ejemplo)\")\n",
    "\n",
    "    # Limpieza básica\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "\n",
    "    required_cols = [\"prompt_original\", \"prompt_paraphrase\", \"prompt_typo_variant\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Faltan columnas requeridas en {csv_path.name}: {missing}\")\n",
    "\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        base_id = str(row[\"id\"])\n",
    "\n",
    "        # Original\n",
    "        records.append({\n",
    "            \"id\": f\"{base_id}_orig\",\n",
    "            \"prompt\": str(row[\"prompt_original\"]).strip(),\n",
    "            \"group_id\": base_id,\n",
    "            \"variation_type\": \"original\",\n",
    "            \"category\": row.get(\"category\", \"\"),\n",
    "            \"difficulty\": row.get(\"difficulty\", \"\"),\n",
    "        })\n",
    "\n",
    "        # Paraphrase\n",
    "        records.append({\n",
    "            \"id\": f\"{base_id}_para\",\n",
    "            \"prompt\": str(row[\"prompt_paraphrase\"]).strip(),\n",
    "            \"group_id\": base_id,\n",
    "            \"variation_type\": \"paraphrase\",\n",
    "            \"category\": row.get(\"category\", \"\"),\n",
    "            \"difficulty\": row.get(\"difficulty\", \"\"),\n",
    "        })\n",
    "\n",
    "        # Typo variant\n",
    "        records.append({\n",
    "            \"id\": f\"{base_id}_typo\",\n",
    "            \"prompt\": str(row[\"prompt_typo_variant\"]).strip(),\n",
    "            \"group_id\": base_id,\n",
    "            \"variation_type\": \"typo\",\n",
    "            \"category\": row.get(\"category\", \"\"),\n",
    "            \"difficulty\": row.get(\"difficulty\", \"\"),\n",
    "        })\n",
    "\n",
    "    long_df = pd.DataFrame(records)\n",
    "    print(f\"→ {len(df)} ejemplos originales → {len(long_df)} filas (x3)\")\n",
    "    return long_df\n",
    "\n",
    "\n",
    "def to_parquet(csv_path: Path, out_dir: Path) -> Path:\n",
    "    df = read_csv_robust(csv_path)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    stem = csv_path.stem\n",
    "\n",
    "    # Detección especial para variation_sensitivity_with_typos\n",
    "    if \"variation_sensitivity_with_typos\" in stem.lower():\n",
    "        df = transform_variation_sensitivity(df, csv_path)\n",
    "        out_name = \"variation_sensitivity.parquet\"   # nombre fijo y limpio\n",
    "    else:\n",
    "        # Comportamiento normal para el resto de datasets\n",
    "        out_name = f\"{stem}.parquet\"\n",
    "\n",
    "    # mantiene el nombre original\n",
    "\n",
    "    pq_path = out_dir / out_name\n",
    "    df.to_parquet(pq_path, index=False)\n",
    "    print(f\"OK: {csv_path.name} → {pq_path.name}\")\n",
    "    return pq_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_files = [p for p in RAW_DIR.rglob(\"*.csv\")]\n",
    "    print(f\"CSVs encontrados: {len(csv_files)}\")\n",
    "    for p in csv_files:\n",
    "        print(f\" - {p.name}\")\n",
    "\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSVs en {RAW_DIR}\")\n",
    "\n",
    "    parquet_paths = []\n",
    "    for csv in csv_files:\n",
    "        try:\n",
    "            pq = to_parquet(csv, OUT_DIR)\n",
    "            parquet_paths.append(pq)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR en {csv.name}: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal Parquets generados: {len(parquet_paths)}\")\n",
    "    for p in parquet_paths:\n",
    "        print(f\"   • {p.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f67300",
   "metadata": {},
   "source": [
    "Test parquets\n",
    "\n",
    "- The aim of this section is to have a space to check the .parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477c4eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>reasoning_close_avg</th>\n",
       "      <th>reasoning_close_delta</th>\n",
       "      <th>reasoning_open_avg</th>\n",
       "      <th>reasoning_open_delta</th>\n",
       "      <th>refusal_correctness_avg</th>\n",
       "      <th>refusal_correctness_delta</th>\n",
       "      <th>summarization_avg</th>\n",
       "      <th>summarization_delta</th>\n",
       "      <th>variation_sensitivity_avg</th>\n",
       "      <th>variation_sensitivity_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cai-llama-3-1-8b-slim</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9266</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6867</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>0.0228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7106</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>0.0251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  reasoning_close_avg  reasoning_close_delta  \\\n",
       "0  cai-llama-3-1-8b-slim                  0.8                    0.4   \n",
       "1                 gpt-4o                  1.0                    0.0   \n",
       "\n",
       "   reasoning_open_avg  reasoning_open_delta  refusal_correctness_avg  \\\n",
       "0              0.9266                0.0133                      1.0   \n",
       "1              0.9281                0.0176                      1.0   \n",
       "\n",
       "   refusal_correctness_delta  summarization_avg  summarization_delta  \\\n",
       "0                        0.0             0.6867               0.0375   \n",
       "1                        0.0             0.7106               0.0336   \n",
       "\n",
       "   variation_sensitivity_avg  variation_sensitivity_delta  \n",
       "0                     0.9657                       0.0228  \n",
       "1                     0.9699                       0.0251  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_outputs = pd.read_parquet(r'C:\\Users\\WJ724NE\\OneDrive - EY\\Documents\\MultiversePipeline\\outputs\\obj_scores.parquet')\n",
    "read_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbf022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>prompt</th>\n",
       "      <th>reference_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>News / Current affairs</td>\n",
       "      <td>As artificial intelligence (AI) technologies c...</td>\n",
       "      <td>The increasing integration of artificial intel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>News / Current affairs</td>\n",
       "      <td>In recent months, education reform has emerged...</td>\n",
       "      <td>Education reform proposals are focusing on add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>News / Current affairs</td>\n",
       "      <td>In recent years, the increasing frequency and ...</td>\n",
       "      <td>Governments are reevaluating cybersecurity leg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>News / Current affairs</td>\n",
       "      <td>As wildfires continue to ravage various region...</td>\n",
       "      <td>Wildfire response efforts are intensifying as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>News / Current affairs</td>\n",
       "      <td>Inflation has emerged as a significant concern...</td>\n",
       "      <td>Inflation is significantly affecting consumers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>General Knowledge</td>\n",
       "      <td>Food webs are intricate networks that illustra...</td>\n",
       "      <td>Food webs illustrate the feeding relationships...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>General Knowledge</td>\n",
       "      <td>The continental drift theory, first proposed b...</td>\n",
       "      <td>The continental drift theory, proposed by Alfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>General Knowledge</td>\n",
       "      <td>Black holes are regions in space where the gra...</td>\n",
       "      <td>Black holes are regions in space with gravitat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>General Knowledge</td>\n",
       "      <td>Probability is a branch of mathematics that de...</td>\n",
       "      <td>Probability is a mathematical framework for qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>General Knowledge</td>\n",
       "      <td>A compass is a navigational instrument that sh...</td>\n",
       "      <td>A compass is a navigational tool that uses a m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                category  \\\n",
       "0     1  News / Current affairs   \n",
       "1     2  News / Current affairs   \n",
       "2     3  News / Current affairs   \n",
       "3     4  News / Current affairs   \n",
       "4     5  News / Current affairs   \n",
       "..  ...                     ...   \n",
       "95   96       General Knowledge   \n",
       "96   97       General Knowledge   \n",
       "97   98       General Knowledge   \n",
       "98   99       General Knowledge   \n",
       "99  100       General Knowledge   \n",
       "\n",
       "                                               prompt  \\\n",
       "0   As artificial intelligence (AI) technologies c...   \n",
       "1   In recent months, education reform has emerged...   \n",
       "2   In recent years, the increasing frequency and ...   \n",
       "3   As wildfires continue to ravage various region...   \n",
       "4   Inflation has emerged as a significant concern...   \n",
       "..                                                ...   \n",
       "95  Food webs are intricate networks that illustra...   \n",
       "96  The continental drift theory, first proposed b...   \n",
       "97  Black holes are regions in space where the gra...   \n",
       "98  Probability is a branch of mathematics that de...   \n",
       "99  A compass is a navigational instrument that sh...   \n",
       "\n",
       "                                    reference_summary  \n",
       "0   The increasing integration of artificial intel...  \n",
       "1   Education reform proposals are focusing on add...  \n",
       "2   Governments are reevaluating cybersecurity leg...  \n",
       "3   Wildfire response efforts are intensifying as ...  \n",
       "4   Inflation is significantly affecting consumers...  \n",
       "..                                                ...  \n",
       "95  Food webs illustrate the feeding relationships...  \n",
       "96  The continental drift theory, proposed by Alfr...  \n",
       "97  Black holes are regions in space with gravitat...  \n",
       "98  Probability is a mathematical framework for qu...  \n",
       "99  A compass is a navigational tool that uses a m...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARQUET_PATH = OUT_DIR / \"summarization.parquet\"\n",
    "\n",
    "read = pd.read_parquet(PARQUET_PATH)\n",
    "read"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
